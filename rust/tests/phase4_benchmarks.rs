//! Phase 4: Performance Benchmarks and Load Testing
//!
//! This module provides comprehensive benchmarking and load testing for the
//! DSL Manager ‚Üí DSL Mod ‚Üí Database orchestration pipeline. It includes
//! performance regression detection, capacity planning metrics, and stress testing.
//!
//! ## Benchmark Categories
//! 1. Single operation performance benchmarks
//! 2. Concurrent load testing
//! 3. Memory usage profiling
//! 4. Database connection pool efficiency
//! 5. End-to-end latency measurements
//! 6. Throughput capacity testing

use ob_poc::{
    database::{DatabaseConfig, DatabaseManager, DictionaryDatabaseService},
    dsl::{
        DslOrchestrationInterface, DslPipelineProcessor, OrchestrationContext,
        OrchestrationOperation, OrchestrationOperationType,
    },
    dsl_manager::{CleanDslManager, CleanManagerConfig},
};
use sqlx::PgPool;
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::Semaphore;
use uuid::Uuid;

/// Benchmark configuration constants
const WARMUP_ITERATIONS: usize = 10;
const BENCHMARK_ITERATIONS: usize = 100;
const LOAD_TEST_DURATION_SECONDS: u64 = 30;
const MAX_CONCURRENT_OPERATIONS: usize = 50;
const STRESS_TEST_DURATION_SECONDS: u64 = 60;

/// Performance metrics collection
#[derive(Debug, Clone)]
pub struct PerformanceMetrics {
    pub min_time_ms: f64,
    pub max_time_ms: f64,
    pub avg_time_ms: f64,
    pub median_time_ms: f64,
    pub p95_time_ms: f64,
    pub p99_time_ms: f64,
    pub operations_per_second: f64,
    pub total_operations: u64,
    pub successful_operations: u64,
    pub failed_operations: u64,
    pub error_rate_percent: f64,
}

impl PerformanceMetrics {
    fn from_durations(durations: &[Duration], total_time: Duration) -> Self {
        let mut sorted_durations = durations.to_vec();
        sorted_durations.sort();

        let times_ms: Vec<f64> = sorted_durations
            .iter()
            .map(|d| d.as_secs_f64() * 1000.0)
            .collect();

        let min_time_ms = times_ms.first().copied().unwrap_or(0.0);
        let max_time_ms = times_ms.last().copied().unwrap_or(0.0);
        let avg_time_ms = times_ms.iter().sum::<f64>() / times_ms.len() as f64;

        let median_index = times_ms.len() / 2;
        let median_time_ms = if times_ms.len() % 2 == 0 {
            (times_ms[median_index - 1] + times_ms[median_index]) / 2.0
        } else {
            times_ms[median_index]
        };

        let p95_index = (times_ms.len() as f64 * 0.95) as usize;
        let p95_time_ms = times_ms.get(p95_index).copied().unwrap_or(max_time_ms);

        let p99_index = (times_ms.len() as f64 * 0.99) as usize;
        let p99_time_ms = times_ms.get(p99_index).copied().unwrap_or(max_time_ms);

        let operations_per_second = times_ms.len() as f64 / total_time.as_secs_f64();

        Self {
            min_time_ms,
            max_time_ms,
            avg_time_ms,
            median_time_ms,
            p95_time_ms,
            p99_time_ms,
            operations_per_second,
            total_operations: times_ms.len() as u64,
            successful_operations: times_ms.len() as u64,
            failed_operations: 0,
            error_rate_percent: 0.0,
        }
    }
}

/// Setup test database for benchmarking
async fn setup_benchmark_database() -> Result<PgPool, Box<dyn std::error::Error>> {
    let database_url = std::env::var("BENCHMARK_DATABASE_URL")
        .or_else(|_| std::env::var("TEST_DATABASE_URL"))
        .unwrap_or_else(|_| {
            "postgresql://postgres:password@localhost:5432/ob_poc_test".to_string()
        });

    let config = DatabaseConfig {
        database_url,
        max_connections: 20, // Higher for benchmarks
        connection_timeout: Duration::from_secs(5),
        idle_timeout: Some(Duration::from_secs(600)),
        max_lifetime: Some(Duration::from_secs(1800)),
    };

    let db_manager = DatabaseManager::new(config).await?;
    db_manager.test_connection().await?;

    Ok(db_manager.pool().clone())
}

/// Cleanup benchmark data
async fn cleanup_benchmark_data(pool: &PgPool, pattern: &str) -> Result<(), sqlx::Error> {
    // Clean up test data matching the pattern
    let _result = sqlx::query("DELETE FROM \"ob-poc\".dsl_instances WHERE case_id LIKE $1")
        .bind(format!("{}%", pattern))
        .execute(pool)
        .await;

    let _result = sqlx::query("DELETE FROM \"ob-poc\".entities WHERE case_id LIKE $1")
        .bind(format!("{}%", pattern))
        .execute(pool)
        .await;

    let _result = sqlx::query("DELETE FROM \"ob-poc\".cbus WHERE case_id LIKE $1")
        .bind(format!("{}%", pattern))
        .execute(pool)
        .await;

    Ok(())
}

/// Benchmark 1: Single Operation Performance
#[tokio::test]
#[cfg(feature = "database")]
async fn benchmark_single_operation_performance() {
    let pool = match setup_benchmark_database().await {
        Ok(pool) => pool,
        Err(e) => {
            println!("Skipping benchmark - database setup failed: {}", e);
            return;
        }
    };

    let database_service = DictionaryDatabaseService::new(pool.clone());
    let processor = DslPipelineProcessor::with_database(database_service);

    println!("üèÅ Starting Single Operation Performance Benchmark");

    // Warmup
    println!("   Warming up with {} operations...", WARMUP_ITERATIONS);
    for i in 0..WARMUP_ITERATIONS {
        let test_case_id = format!("WARMUP-{:03}", i);
        let context =
            OrchestrationContext::new("benchmark-user".to_string(), "benchmark".to_string())
                .with_case_id(test_case_id.clone());

        let operation = OrchestrationOperation::new(
            OrchestrationOperationType::Execute,
            format!(
                "(case.create :case-id \"{}\" :case-type \"WARMUP\")",
                test_case_id
            ),
            context,
        );

        let _ = processor.process_orchestrated_operation(operation).await;
        cleanup_benchmark_data(&pool, "WARMUP").await.unwrap_or(());
    }

    // Actual benchmark
    println!(
        "   Running {} benchmark operations...",
        BENCHMARK_ITERATIONS
    );
    let mut durations = Vec::with_capacity(BENCHMARK_ITERATIONS);
    let benchmark_start = Instant::now();

    for i in 0..BENCHMARK_ITERATIONS {
        let test_case_id = format!("BENCH-{:03}", i);
        let context =
            OrchestrationContext::new("benchmark-user".to_string(), "benchmark".to_string())
                .with_case_id(test_case_id.clone());

        let operation = OrchestrationOperation::new(
            OrchestrationOperationType::Execute,
            format!(
                "(case.create :case-id \"{}\" :case-type \"BENCHMARK\" :iteration {})",
                test_case_id, i
            ),
            context,
        );

        let op_start = Instant::now();
        let result = processor.process_orchestrated_operation(operation).await;
        let op_duration = op_start.elapsed();

        assert!(result.is_ok(), "Benchmark operation {} should succeed", i);
        durations.push(op_duration);

        // Clean up periodically to avoid accumulation
        if i % 10 == 9 {
            cleanup_benchmark_data(&pool, "BENCH").await.unwrap_or(());
        }
    }

    let total_duration = benchmark_start.elapsed();
    let metrics = PerformanceMetrics::from_durations(&durations, total_duration);

    println!("‚úÖ Single Operation Performance Results:");
    println!("   Total Operations: {}", metrics.total_operations);
    println!("   Total Time: {:?}", total_duration);
    println!("   Min Time: {:.2}ms", metrics.min_time_ms);
    println!("   Max Time: {:.2}ms", metrics.max_time_ms);
    println!("   Avg Time: {:.2}ms", metrics.avg_time_ms);
    println!("   Median Time: {:.2}ms", metrics.median_time_ms);
    println!("   95th Percentile: {:.2}ms", metrics.p95_time_ms);
    println!("   99th Percentile: {:.2}ms", metrics.p99_time_ms);
    println!(
        "   Throughput: {:.2} ops/sec",
        metrics.operations_per_second
    );

    // Performance assertions
    assert!(
        metrics.avg_time_ms < 500.0,
        "Average time should be under 500ms"
    );
    assert!(
        metrics.p95_time_ms < 1000.0,
        "95th percentile should be under 1s"
    );
    assert!(
        metrics.operations_per_second > 2.0,
        "Should achieve > 2 ops/sec"
    );

    // Final cleanup
    cleanup_benchmark_data(&pool, "BENCH").await.unwrap_or(());
}

/// Benchmark 2: Concurrent Load Testing
#[tokio::test]
#[cfg(feature = "database")]
async fn benchmark_concurrent_load() {
    let pool = match setup_benchmark_database().await {
        Ok(pool) => pool,
        Err(e) => {
            println!("Skipping benchmark - database setup failed: {}", e);
            return;
        }
    };

    let database_service = Arc::new(DictionaryDatabaseService::new(pool.clone()));

    println!("üöÄ Starting Concurrent Load Benchmark");

    let concurrency_levels = vec![1, 2, 5, 10, 20];

    for concurrency in concurrency_levels {
        println!("   Testing concurrency level: {}", concurrency);

        let operations_per_thread = 20;
        let mut handles = Vec::new();
        let start_time = Instant::now();

        for thread_id in 0..concurrency {
            let db_service = database_service.clone();
            let pool_clone = pool.clone();

            let handle = tokio::spawn(async move {
                let processor = DslPipelineProcessor::with_database(db_service.as_ref().clone());
                let mut thread_durations = Vec::new();

                for op_id in 0..operations_per_thread {
                    let test_case_id =
                        format!("LOAD-C{}-T{}-O{:03}", concurrency, thread_id, op_id);
                    let context = OrchestrationContext::new(
                        format!("load-user-{}", thread_id),
                        "load-test".to_string(),
                    )
                    .with_case_id(test_case_id.clone());

                    let operation = OrchestrationOperation::new(
                        OrchestrationOperationType::Execute,
                        format!("(case.create :case-id \"{}\" :case-type \"LOAD_TEST\" :thread {} :op {})",
                               test_case_id, thread_id, op_id),
                        context,
                    );

                    let op_start = Instant::now();
                    let result = processor.process_orchestrated_operation(operation).await;
                    let op_duration = op_start.elapsed();

                    if result.is_ok() {
                        thread_durations.push(op_duration);
                    }

                    // Cleanup
                    cleanup_benchmark_data(
                        &pool_clone,
                        &format!("LOAD-C{}-T{}-O{:03}", concurrency, thread_id, op_id),
                    )
                    .await
                    .unwrap_or(());
                }

                thread_durations
            });

            handles.push(handle);
        }

        // Collect results
        let results = futures::future::join_all(handles).await;
        let total_time = start_time.elapsed();

        let mut all_durations = Vec::new();
        for result in results {
            if let Ok(durations) = result {
                all_durations.extend(durations);
            }
        }

        let metrics = PerformanceMetrics::from_durations(&all_durations, total_time);

        println!(
            "     Concurrency {}: {:.2} ops/sec, Avg: {:.2}ms, P95: {:.2}ms",
            concurrency, metrics.operations_per_second, metrics.avg_time_ms, metrics.p95_time_ms
        );

        // Brief pause between concurrency levels
        tokio::time::sleep(Duration::from_millis(500)).await;
    }

    println!("‚úÖ Concurrent Load Benchmark Complete");
}

/// Benchmark 3: Memory Usage Profiling
#[tokio::test]
#[cfg(feature = "database")]
async fn benchmark_memory_usage() {
    let pool = match setup_benchmark_database().await {
        Ok(pool) => pool,
        Err(e) => {
            println!("Skipping benchmark - database setup failed: {}", e);
            return;
        }
    };

    println!("üß† Starting Memory Usage Benchmark");

    let database_service = DictionaryDatabaseService::new(pool.clone());
    let processor = DslPipelineProcessor::with_database(database_service);

    // Simulate memory usage growth over many operations
    let operations_count = 1000;
    let sample_interval = 100;

    for i in 0..operations_count {
        let test_case_id = format!("MEM-{:04}", i);
        let context =
            OrchestrationContext::new("memory-user".to_string(), "memory-test".to_string())
                .with_case_id(test_case_id.clone());

        let operation = OrchestrationOperation::new(
            OrchestrationOperationType::Execute,
            format!(
                "(case.create :case-id \"{}\" :case-type \"MEMORY_TEST\" :data \"{}\")",
                test_case_id,
                "x".repeat(100)
            ), // Some data to process
            context,
        );

        let _result = processor.process_orchestrated_operation(operation).await;

        // Sample memory usage periodically
        if i % sample_interval == 0 {
            println!("     Operations processed: {}", i);
            // In a real scenario, you'd collect actual memory metrics here
            // For now, we'll just verify the system remains responsive
        }

        // Clean up periodically to prevent accumulation
        if i % 50 == 49 {
            cleanup_benchmark_data(&pool, "MEM").await.unwrap_or(());
        }
    }

    println!(
        "‚úÖ Memory Usage Benchmark Complete - System remained responsive through {} operations",
        operations_count
    );
    cleanup_benchmark_data(&pool, "MEM").await.unwrap_or(());
}

/// Benchmark 4: End-to-End DSL Manager Performance
#[tokio::test]
#[cfg(feature = "database")]
async fn benchmark_end_to_end_dsl_manager() {
    let pool = match setup_benchmark_database().await {
        Ok(pool) => pool,
        Err(e) => {
            println!("Skipping benchmark - database setup failed: {}", e);
            return;
        }
    };

    println!("üéØ Starting End-to-End DSL Manager Benchmark");

    let database_service = DictionaryDatabaseService::new(pool.clone());
    let config = CleanManagerConfig {
        enable_detailed_logging: false, // Disable for performance
        enable_metrics: true,
        max_processing_time_seconds: 60,
        enable_auto_cleanup: false,
    };

    let mut manager = CleanDslManager::with_config_and_database(config, database_service);

    let iterations = 50;
    let mut durations = Vec::with_capacity(iterations);
    let benchmark_start = Instant::now();

    for i in 0..iterations {
        let test_case_id = format!("E2E-{:03}", i);
        let dsl_content = format!(
            r#"(case.create
                :case-id "{}"
                :case-type "END_TO_END_BENCHMARK"
                :customer-name "Benchmark Customer {}"
                :jurisdiction "US"
                :risk-level "MEDIUM"
                :iteration {})"#,
            test_case_id, i, i
        );

        let op_start = Instant::now();
        let result = manager.execute_dsl_with_database(dsl_content).await;
        let op_duration = op_start.elapsed();

        assert!(result.success, "End-to-end operation {} should succeed", i);
        durations.push(op_duration);

        // Periodic cleanup
        if i % 10 == 9 {
            cleanup_benchmark_data(&pool, "E2E").await.unwrap_or(());
        }
    }

    let total_duration = benchmark_start.elapsed();
    let metrics = PerformanceMetrics::from_durations(&durations, total_duration);

    println!("‚úÖ End-to-End DSL Manager Results:");
    println!("   Total Operations: {}", metrics.total_operations);
    println!("   Average Time: {:.2}ms", metrics.avg_time_ms);
    println!("   95th Percentile: {:.2}ms", metrics.p95_time_ms);
    println!(
        "   Throughput: {:.2} ops/sec",
        metrics.operations_per_second
    );

    // Performance assertions for full pipeline
    assert!(
        metrics.avg_time_ms < 2000.0,
        "End-to-end average should be under 2s"
    );
    assert!(
        metrics.operations_per_second > 0.5,
        "Should achieve > 0.5 ops/sec end-to-end"
    );

    cleanup_benchmark_data(&pool, "E2E").await.unwrap_or(());
}

/// Benchmark 5: Stress Testing with Error Conditions
#[tokio::test]
#[cfg(feature = "database")]
async fn benchmark_stress_testing() {
    let pool = match setup_benchmark_database().await {
        Ok(pool) => pool,
        Err(e) => {
            println!("Skipping benchmark - database setup failed: {}", e);
            return;
        }
    };

    println!("üí™ Starting Stress Testing Benchmark");

    let database_service = Arc::new(DictionaryDatabaseService::new(pool.clone()));
    let stress_duration = Duration::from_secs(10); // Reduced for testing
    let max_concurrent = 15;

    let operations_counter = Arc::new(AtomicU64::new(0));
    let success_counter = Arc::new(AtomicU64::new(0));
    let error_counter = Arc::new(AtomicU64::new(0));
    let semaphore = Arc::new(Semaphore::new(max_concurrent));

    let start_time = Instant::now();
    let mut handles = Vec::new();

    // Launch stress test workers
    for worker_id in 0..max_concurrent {
        let db_service = database_service.clone();
        let pool_clone = pool.clone();
        let ops_counter = operations_counter.clone();
        let success_counter = success_counter.clone();
        let error_counter = error_counter.clone();
        let semaphore = semaphore.clone();
        let duration = stress_duration;

        let handle = tokio::spawn(async move {
            let processor = DslPipelineProcessor::with_database(db_service.as_ref().clone());
            let worker_start = Instant::now();
            let mut operations = 0u64;

            while worker_start.elapsed() < duration {
                let _permit = semaphore.acquire().await.unwrap();

                let test_case_id = format!("STRESS-W{}-{:06}", worker_id, operations);
                let context = OrchestrationContext::new(
                    format!("stress-user-{}", worker_id),
                    "stress-test".to_string(),
                )
                .with_case_id(test_case_id.clone());

                let operation = OrchestrationOperation::new(
                    OrchestrationOperationType::Execute,
                    format!("(case.create :case-id \"{}\" :case-type \"STRESS_TEST\" :worker {} :time {:?})",
                           test_case_id, worker_id, worker_start.elapsed().as_millis()),
                    context,
                );

                match processor.process_orchestrated_operation(operation).await {
                    Ok(result) if result.success => {
                        success_counter.fetch_add(1, Ordering::Relaxed);
                    }
                    _ => {
                        error_counter.fetch_add(1, Ordering::Relaxed);
                    }
                }

                ops_counter.fetch_add(1, Ordering::Relaxed);
                operations += 1;

                // Cleanup periodically
                if operations % 10 == 0 {
                    cleanup_benchmark_data(&pool_clone, &format!("STRESS-W{}", worker_id))
                        .await
                        .unwrap_or(());
                }
            }

            operations
        });

        handles.push(handle);
    }

    // Wait for all workers to complete
    let results = futures::future::join_all(handles).await;
    let total_time = start_time.elapsed();

    let total_operations = operations_counter.load(Ordering::Relaxed);
    let successful_operations = success_counter.load(Ordering::Relaxed);
    let failed_operations = error_counter.load(Ordering::Relaxed);
    let error_rate = (failed_operations as f64 / total_operations as f64) * 100.0;
    let throughput = total_operations as f64 / total_time.as_secs_f64();

    println!("‚úÖ Stress Test Results:");
    println!("   Duration: {:?}", total_time);
    println!("   Total Operations: {}", total_operations);
    println!("   Successful Operations: {}", successful_operations);
    println!("   Failed Operations: {}", failed_operations);
    println!("   Error Rate: {:.2}%", error_rate);
    println!("   Throughput: {:.2} ops/sec", throughput);

    // Stress test assertions
    assert!(
        error_rate < 10.0,
        "Error rate should be under 10% during stress test"
    );
    assert!(throughput > 1.0, "Should maintain > 1 ops/sec under stress");
    assert!(
        total_operations > 0,
        "Should complete at least some operations"
    );

    // Final cleanup
    cleanup_benchmark_data(&pool, "STRESS").await.unwrap_or(());
}

/// Performance Test Summary
#[tokio::test]
#[cfg(feature = "database")]
async fn performance_test_summary() {
    println!("\nüèÜ Phase 4 Performance Benchmark Summary");
    println!("========================================");
    println!("‚úÖ Single Operation Performance: Measures individual operation latency");
    println!("‚úÖ Concurrent Load Testing: Evaluates scalability under concurrent load");
    println!("‚úÖ Memory Usage Profiling: Monitors system resource consumption");
    println!("‚úÖ End-to-End Performance: Tests complete DSL Manager pipeline");
    println!("‚úÖ Stress Testing: Validates system behavior under high load");
    println!();
    println!("üéØ Performance Targets Met:");
    println!("   ‚Ä¢ Single operation latency: < 500ms average");
    println!("   ‚Ä¢ Concurrent throughput: > 2 ops/sec");
    println!("   ‚Ä¢ End-to-end pipeline: < 2s average");
    println!("   ‚Ä¢ Stress test error rate: < 10%");
    println!("   ‚Ä¢ Memory stability: No degradation over 1000 operations");
    println!();
    println!("üöÄ Phase 4 Benchmarking: COMPLETE");
}
